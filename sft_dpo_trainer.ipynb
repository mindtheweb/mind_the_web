{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a421a02a-7bd8-4b18-8804-3205e03141d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForLanguageModeling, \n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    TaskType, \n",
    "    prepare_model_for_kbit_training, \n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "from trl import (\n",
    "    AutoModelForCausalLMWithValueHead,\n",
    "    SFTTrainer, \n",
    "    SFTConfig,\n",
    "    DPOTrainer, \n",
    "    DPOConfig\n",
    ")\n",
    "\n",
    "POLICY_NAME = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(POLICY_NAME, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "#quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f333a86",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "# STAGE 1: SFT TRAINING\n",
    "####################################\n",
    "\n",
    "# -----------------------\n",
    "# Load SFT dataset\n",
    "# -----------------------\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(l) for l in f if l.strip()]\n",
    "\n",
    "sft_data = load_jsonl(\"sft_dataset.jsonl\")\n",
    "sft_dataset = Dataset.from_list(sft_data)\n",
    "\n",
    "# -----------------------\n",
    "# Tokenize SFT data\n",
    "# -----------------------\n",
    "def tokenize_fn(batch):\n",
    "    texts = [\n",
    "        p + \"\\n\" + pred\n",
    "        for p, pred in zip(batch[\"prompt\"], batch[\"prediction\"])\n",
    "    ]\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized_sft = sft_dataset.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=[\"prompt\", \"prediction\"]\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Split into train/eval (90/10)\n",
    "# -----------------------\n",
    "split = tokenized_sft.train_test_split(test_size=0.1, seed=42)\n",
    "train_sft = split[\"train\"]\n",
    "eval_sft = split[\"test\"]\n",
    "\n",
    "print(f\"SFT - Train size: {len(train_sft)} | Eval size: {len(eval_sft)}\")\n",
    "\n",
    "# -----------------------\n",
    "# Load base model for SFT\n",
    "# -----------------------\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    POLICY_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "base_model = prepare_model_for_kbit_training(base_model, use_gradient_checkpointing=True)\n",
    "\n",
    "# -----------------------\n",
    "# Add LoRA for SFT\n",
    "# -----------------------\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    ")\n",
    "\n",
    "sft_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# -----------------------\n",
    "# SFT Training setup\n",
    "# -----------------------\n",
    "sft_training_args = TrainingArguments(\n",
    "    output_dir=\"./sft_out\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1, \n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=10,\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    dataloader_drop_last=False,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "sft_trainer = Trainer(\n",
    "    model=sft_model,\n",
    "    args=sft_training_args,\n",
    "    train_dataset=train_sft,\n",
    "    eval_dataset=eval_sft,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "sft_trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=7, early_stopping_threshold=0.0))\n",
    "\n",
    "# -----------------------\n",
    "# Train SFT\n",
    "# -----------------------\n",
    "print(\"Starting SFT training...\")\n",
    "sft_trainer.train()\n",
    "\n",
    "# -----------------------\n",
    "# Save SFT adapter\n",
    "# -----------------------\n",
    "sft_model.save_pretrained(\"./sft_adapter\")\n",
    "tokenizer.save_pretrained(\"./sft_adapter\")\n",
    "\n",
    "print(\"SFT training finished. Adapter saved to ./sft_adapter\")\n",
    "\n",
    "# Clean up SFT model to free memory\n",
    "del sft_model, sft_trainer, base_model\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e42367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import psutil\n",
    "import os\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "def debug_memory_usage(stage_name: str, model: Optional[Any] = None, detailed: bool = True) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Debug memory usage at any stage of training.\n",
    "    \n",
    "    Args:\n",
    "        stage_name: Name of the current stage for logging\n",
    "        model: Optional model to analyze parameter counts\n",
    "        detailed: Whether to show detailed breakdown\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with memory statistics\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    stats = {}\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        stats['gpu_allocated_gb'] = torch.cuda.memory_allocated() / 1e9\n",
    "        stats['gpu_reserved_gb'] = torch.cuda.memory_reserved() / 1e9\n",
    "        stats['gpu_free_gb'] = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved()) / 1e9\n",
    "        stats['gpu_total_gb'] = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        stats['gpu_utilization_pct'] = (stats['gpu_allocated_gb'] / stats['gpu_total_gb']) * 100\n",
    "        \n",
    "        if detailed:\n",
    "            memory_summary = torch.cuda.memory_summary()\n",
    "            stats['gpu_memory_summary'] = memory_summary\n",
    "    \n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    stats['ram_used_gb'] = memory_info.rss / 1e9\n",
    "    stats['ram_virtual_gb'] = memory_info.vms / 1e9\n",
    "    \n",
    "    system_memory = psutil.virtual_memory()\n",
    "    stats['ram_total_gb'] = system_memory.total / 1e9\n",
    "    stats['ram_available_gb'] = system_memory.available / 1e9\n",
    "    stats['ram_utilization_pct'] = system_memory.percent\n",
    "    \n",
    "    if model is not None:\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        stats['model_total_params'] = total_params\n",
    "        stats['model_trainable_params'] = trainable_params\n",
    "        stats['model_trainable_pct'] = (trainable_params / total_params) * 100 if total_params > 0 else 0\n",
    "        \n",
    "\n",
    "        stats['model_params_memory_gb'] = (total_params * 2) / 1e9\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MEMORY DEBUG - {stage_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   GPU Memory:\")\n",
    "        print(f\"   Allocated: {stats['gpu_allocated_gb']:.2f} GB\")\n",
    "        print(f\"   Reserved:  {stats['gpu_reserved_gb']:.2f} GB\")\n",
    "        print(f\"   Free:      {stats['gpu_free_gb']:.2f} GB\")\n",
    "        print(f\"   Total:     {stats['gpu_total_gb']:.2f} GB\")\n",
    "        print(f\"   Usage:     {stats['gpu_utilization_pct']:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n  System RAM:\")\n",
    "    print(f\"   Process:   {stats['ram_used_gb']:.2f} GB\")\n",
    "    print(f\"   Available: {stats['ram_available_gb']:.2f} GB\")\n",
    "    print(f\"   Total:     {stats['ram_total_gb']:.2f} GB\")\n",
    "    print(f\"   Usage:     {stats['ram_utilization_pct']:.1f}%\")\n",
    "    \n",
    "    if model is not None:\n",
    "        print(f\"\\n  Model Stats:\")\n",
    "        print(f\"   Total params:     {stats['model_total_params']:,}\")\n",
    "        print(f\"   Trainable params: {stats['model_trainable_params']:,}\")\n",
    "        print(f\"   Trainable %:      {stats['model_trainable_pct']:.2f}%\")\n",
    "        print(f\"   Est. param memory: {stats['model_params_memory_gb']:.2f} GB\")\n",
    "    \n",
    "    if detailed and torch.cuda.is_available():\n",
    "        print(f\"\\n  Detailed GPU Memory Breakdown:\")\n",
    "        print(stats['gpu_memory_summary'])\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def track_memory_through_stages():\n",
    "    \"\"\"\n",
    "    Track memory usage through all stages of your DPO pipeline.\n",
    "    Insert calls to this function at each major stage.\n",
    "    \"\"\"\n",
    "    \n",
    "    stages = []\n",
    "    \n",
    "    def log_stage(stage_name: str, model=None, detailed=False):\n",
    "        stats = debug_memory_usage(stage_name, model, detailed)\n",
    "        stages.append({\n",
    "            'stage': stage_name,\n",
    "            'stats': stats,\n",
    "            'timestamp': torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None\n",
    "        })\n",
    "        return stats\n",
    "    \n",
    "    return log_stage, stages\n",
    "\n",
    "def compare_memory_stages(stages_data):\n",
    "    \"\"\"\n",
    "    Compare memory usage across different stages.\n",
    "    \n",
    "    Args:\n",
    "        stages_data: List of stage data from track_memory_through_stages\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"MEMORY USAGE COMPARISON ACROSS STAGES\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"{'Stage':<25} {'GPU Alloc (GB)':<15} {'GPU Reserved (GB)':<18} {'RAM Used (GB)':<15} {'Model Params':<15}\")\n",
    "    print(f\"{'-'*25} {'-'*15} {'-'*18} {'-'*15} {'-'*15}\")\n",
    "    \n",
    "    for stage_data in stages_data:\n",
    "        stage = stage_data['stage']\n",
    "        stats = stage_data['stats']\n",
    "        \n",
    "        gpu_alloc = f\"{stats.get('gpu_allocated_gb', 0):.2f}\" if torch.cuda.is_available() else \"N/A\"\n",
    "        gpu_reserved = f\"{stats.get('gpu_reserved_gb', 0):.2f}\" if torch.cuda.is_available() else \"N/A\"\n",
    "        ram_used = f\"{stats['ram_used_gb']:.2f}\"\n",
    "        model_params = f\"{stats.get('model_total_params', 0):,}\" if stats.get('model_total_params') else \"N/A\"\n",
    "        \n",
    "        print(f\"{stage:<25} {gpu_alloc:<15} {gpu_reserved:<18} {ram_used:<15} {model_params:<15}\")\n",
    "    \n",
    "    if len(stages_data) > 1:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(\"MEMORY DELTAS (Change from previous stage)\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        for i in range(1, len(stages_data)):\n",
    "            curr = stages_data[i]['stats']\n",
    "            prev = stages_data[i-1]['stats']\n",
    "            stage = stages_data[i]['stage']\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                gpu_delta = curr.get('gpu_allocated_gb', 0) - prev.get('gpu_allocated_gb', 0)\n",
    "                print(f\"{stage}: GPU +{gpu_delta:.2f} GB\")\n",
    "            \n",
    "            ram_delta = curr['ram_used_gb'] - prev['ram_used_gb']\n",
    "            print(f\"{stage}: RAM +{ram_delta:.2f} GB\")\n",
    "\n",
    "def debug_dpo_pipeline():\n",
    "    \"\"\"\n",
    "    Example of how to integrate memory debugging into your DPO pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    log_memory, stages = track_memory_through_stages()\n",
    "    \n",
    "    print(\" DEBUGGING YOUR DPO PIPELINE MEMORY USAGE\")\n",
    "    \n",
    "    # Stage 0: Initial state\n",
    "    log_memory(\"Initial State\")\n",
    "\n",
    "    \n",
    "    return log_memory, stages\n",
    "\n",
    "def verify_model_device_placement(models_dict: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Verify where each model is actually placed in memory.\n",
    "    \n",
    "    Args:\n",
    "        models_dict: Dictionary of {'model_name': model} to check\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"MODEL DEVICE PLACEMENT VERIFICATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for model_name, model in models_dict.items():\n",
    "        print(f\"\\n {model_name}:\")\n",
    "        \n",
    "        if hasattr(model, 'parameters'):\n",
    "            devices = set()\n",
    "            param_count_by_device = {}\n",
    "            memory_by_device = {}\n",
    "            \n",
    "            for name, param in model.named_parameters():\n",
    "                device = str(param.device)\n",
    "                devices.add(device)\n",
    "                \n",
    "                if device not in param_count_by_device:\n",
    "                    param_count_by_device[device] = 0\n",
    "                    memory_by_device[device] = 0\n",
    "                \n",
    "                param_count_by_device[device] += param.numel()\n",
    "                memory_by_device[device] += param.numel() * param.element_size()\n",
    "            \n",
    "            print(f\"   Devices found: {devices}\")\n",
    "            for device in devices:\n",
    "                param_count = param_count_by_device[device]\n",
    "                memory_gb = memory_by_device[device] / 1e9\n",
    "                print(f\"   {device}: {param_count:,} params, {memory_gb:.2f} GB\")\n",
    "        else:\n",
    "            print(f\"   No parameters found (not a model?)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f9be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# STAGE 2: DPO TRAINING WITH MEMORY DEBUG\n",
    "####################################\n",
    "\n",
    "# Initialize memory tracking\n",
    "log_memory, stages = track_memory_through_stages()\n",
    "\n",
    "log_memory(\"00_Initial_State\")\n",
    "\n",
    "# -----------------------\n",
    "# Load DPO dataset\n",
    "# -----------------------\n",
    "print(\"Loading DPO dataset...\")\n",
    "with open(\"dpo_pairs_clean.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dpo_pairs = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Loaded {len(dpo_pairs)} DPO pairs\")\n",
    "\n",
    "hf_dpo = Dataset.from_list(dpo_pairs)\n",
    "split_dpo = hf_dpo.train_test_split(test_size=0.02, seed=42)\n",
    "train_dpo, eval_dpo = split_dpo[\"train\"], split_dpo[\"test\"]\n",
    "print(f\"DPO - Train size: {len(train_dpo)} | Eval size: {len(eval_dpo)}\")\n",
    "\n",
    "log_memory(\"01_After_Dataset_Load\")\n",
    "\n",
    "# -----------------------\n",
    "# Load base model for DPO\n",
    "# -----------------------\n",
    "print(\"Loading base model for DPO in full precision...\")\n",
    "dpo_base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    POLICY_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "log_memory(\"02_After_Base_Model_Load\", dpo_base_model, detailed=True)\n",
    "\n",
    "# -----------------------\n",
    "# Load and merge SFT adapter\n",
    "# -----------------------\n",
    "print(\"Loading SFT adapter...\")\n",
    "dpo_model = PeftModel.from_pretrained(dpo_base_model, \"./sft_adapter\")\n",
    "\n",
    "log_memory(\"03_After_SFT_Adapter_Load\", dpo_model)\n",
    "\n",
    "print(\"Merging SFT adapter with base model...\")\n",
    "dpo_model = dpo_model.merge_and_unload()\n",
    "\n",
    "log_memory(\"04_After_SFT_Merge\", dpo_model)\n",
    "\n",
    "# -----------------------\n",
    "# Create frozen reference model\n",
    "# -----------------------\n",
    "print(\"Creating frozen reference model...\")\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "    POLICY_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "log_memory(\"05_After_Reference_Model_Creation\", dpo_model)\n",
    "\n",
    "ref_model = PeftModel.from_pretrained(ref_model, \"./sft_adapter\")\n",
    "ref_model = ref_model.merge_and_unload()\n",
    "\n",
    "log_memory(\"06_After_Reference_SFT_Merge\", dpo_model)\n",
    "\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "ref_model.eval()\n",
    "print(\"Frozen reference model ready\")\n",
    "\n",
    "log_memory(\"07_After_Reference_Finalized\", dpo_model)\n",
    "\n",
    "print(\"\\n VERIFYING ACTUAL DEVICE PLACEMENT:\")\n",
    "verify_model_device_placement({\n",
    "    'policy_model': dpo_model,\n",
    "    'reference_model': ref_model\n",
    "})\n",
    "\n",
    "# -----------------------\n",
    "# Add fresh LoRA adapters to policy model\n",
    "# -----------------------\n",
    "print(\"Adding fresh LoRA adapters for DPO...\")\n",
    "dpo_lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")\n",
    "\n",
    "dpo_model = get_peft_model(dpo_model, dpo_lora_config)\n",
    "\n",
    "log_memory(\"08_After_LoRA_Addition\", dpo_model, detailed=True)\n",
    "\n",
    "if hasattr(dpo_model, 'gradient_checkpointing_disable'):\n",
    "    dpo_model.gradient_checkpointing_disable()\n",
    "elif hasattr(dpo_model, 'config'):\n",
    "    dpo_model.config.use_cache = False\n",
    "    if hasattr(dpo_model.config, 'gradient_checkpointing'):\n",
    "        dpo_model.config.gradient_checkpointing = False\n",
    "\n",
    "dpo_model.train()\n",
    "print(\"Setting model to training mode...\")\n",
    "\n",
    "dpo_model.config.use_cache = False\n",
    "if hasattr(dpo_model.config, 'return_dict'):\n",
    "    dpo_model.config.return_dict = True\n",
    "\n",
    "log_memory(\"09_After_Model_Configuration\", dpo_model)\n",
    "\n",
    "# -----------------------\n",
    "# Verify trainable parameters\n",
    "# -----------------------\n",
    "print(\"Verifying trainable parameters...\")\n",
    "trainable_params = []\n",
    "for name, param in dpo_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        trainable_params.append(name)\n",
    "\n",
    "if len(trainable_params) > 0:\n",
    "    print(f\"Found {len(trainable_params)} trainable parameters\")\n",
    "    print(\"First few trainable params:\")\n",
    "    for i, name in enumerate(trainable_params[:5]):\n",
    "        print(f\"   â€¢ {name}\")\n",
    "    if len(trainable_params) > 5:\n",
    "        print(f\"   â€¢ ... and {len(trainable_params) - 5} more\")\n",
    "else:\n",
    "    print(\"ERROR: No trainable parameters found!\")\n",
    "    raise ValueError(\"No trainable parameters - model setup failed\")\n",
    "\n",
    "eval_subset = eval_dpo.shuffle(seed=42).select(range(50))\n",
    "\n",
    "# -----------------------\n",
    "# DPO config with reference model and stronger hyperparameters\n",
    "# -----------------------\n",
    "dpo_args = DPOConfig(\n",
    "    output_dir=\"./dpo_out\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=5e-6,\n",
    "    num_train_epochs=2,\n",
    "    beta=0.2,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    "    load_best_model_at_end=True, \n",
    "    metric_for_best_model=\"eval_loss\", \n",
    "    greater_is_better=False,\n",
    "    \n",
    "    max_length=786,\n",
    "    max_prompt_length=512,\n",
    "    precompute_ref_log_probs=False,\n",
    "    remove_unused_columns=True,\n",
    "    dataloader_drop_last=True,\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_pin_memory=False,\n",
    "    \n",
    "\n",
    "    prediction_loss_only=True,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Create DPO Trainer with reference model\n",
    "# -----------------------\n",
    "print(\"Creating DPO trainer with reference model...\")\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=dpo_model,\n",
    "    ref_model=ref_model,\n",
    "    train_dataset=train_dpo,\n",
    "    eval_dataset=eval_dpo,\n",
    "    processing_class=tokenizer,\n",
    "    args=dpo_args,\n",
    ")\n",
    "\n",
    "log_memory(\"10_After_DPO_Trainer_Creation\", dpo_model, detailed=True)\n",
    "\n",
    "dpo_trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=3))\n",
    "\n",
    "# -----------------------\n",
    "# Training info\n",
    "# -----------------------\n",
    "total_steps = len(train_dpo) // dpo_args.per_device_train_batch_size // dpo_args.gradient_accumulation_steps * dpo_args.num_train_epochs\n",
    "print(f\"Training Summary:\")\n",
    "print(f\"   â€¢ Dataset: {len(train_dpo):,} train samples\")\n",
    "print(f\"   â€¢ Batch size: {dpo_args.per_device_train_batch_size}\")\n",
    "print(f\"   â€¢ Gradient accumulation: {dpo_args.gradient_accumulation_steps}\")\n",
    "print(f\"   â€¢ Effective batch size: {dpo_args.per_device_train_batch_size * dpo_args.gradient_accumulation_steps}\")\n",
    "print(f\"   â€¢ Epochs: {dpo_args.num_train_epochs}\")\n",
    "print(f\"   â€¢ Estimated steps: ~{total_steps:,}\")\n",
    "\n",
    "print(\"Starting DPO training with reference model...\")\n",
    "\n",
    "# -----------------------\n",
    "# Final gradient check and fix\n",
    "# -----------------------\n",
    "print(\"Final gradient check and fix...\")\n",
    "dpo_model.train()\n",
    "\n",
    "lora_param_count = 0\n",
    "for name, param in dpo_model.named_parameters():\n",
    "    if 'lora' in name.lower():\n",
    "        param.requires_grad = True\n",
    "        lora_param_count += 1\n",
    "\n",
    "# Test gradient computation\n",
    "dpo_model.zero_grad()\n",
    "test_input = tokenizer(\"Test gradient\", return_tensors=\"pt\", max_length=50, truncation=True).to(\"cuda:0\")\n",
    "test_output = dpo_model(**test_input)\n",
    "test_loss = test_output.logits.sum()\n",
    "test_loss.backward()\n",
    "\n",
    "has_grad = any(param.grad is not None for param in dpo_model.parameters() if param.requires_grad)\n",
    "print(f\"Gradient test: {'PASSED' if has_grad else 'FAILED'}\")\n",
    "\n",
    "if not has_grad:\n",
    "    print(\"Still no gradients - there may be a deeper issue\")\n",
    "    grad_params = [name for name, param in dpo_model.named_parameters() if param.requires_grad]\n",
    "    print(f\"Parameters with requires_grad=True: {len(grad_params)}\")\n",
    "else:\n",
    "    print(\"Gradients working - proceeding with training\")\n",
    "\n",
    "log_memory(\"11_Before_Training_Start\", dpo_model, detailed=True)\n",
    "\n",
    "print(\"Starting DPO training...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" COMPLETE MEMORY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "compare_memory_stages(stages)\n",
    "\n",
    "# -----------------------\n",
    "# Add memory monitoring during training\n",
    "# -----------------------\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class MemoryMonitorCallback(TrainerCallback):\n",
    "    def __init__(self, log_memory_func):\n",
    "        self.log_memory = log_memory_func\n",
    "        \n",
    "    def on_step_begin(self, args, state, control, model=None, **kwargs):\n",
    "        if state.global_step % 10 == 0:  # Every 10 steps to reduce spam\n",
    "            self.log_memory(f\"Training_Step_{state.global_step}\", model)\n",
    "        return control\n",
    "        \n",
    "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
    "        if state.global_step == 1:  # Log after first step\n",
    "            self.log_memory(f\"After_First_Step\", model)\n",
    "        return control\n",
    "\n",
    "memory_callback = MemoryMonitorCallback(log_memory)\n",
    "dpo_trainer.add_callback(memory_callback)\n",
    "\n",
    "print(\"Memory monitoring callback added successfully!\")\n",
    "\n",
    "# -----------------------\n",
    "# Train DPO\n",
    "# -----------------------\n",
    "try:\n",
    "    dpo_trainer.train()\n",
    "except torch.cuda.OutOfMemoryError as e:\n",
    "    print(f\"\\nðŸ’¥ OOM ERROR AT TRAINING START!\")\n",
    "    print(f\"Error: {e}\")\n",
    "    log_memory(\"OOM_Error_Point\", dpo_model, detailed=True)\n",
    "\n",
    "    compare_memory_stages(stages)\n",
    "    \n",
    "    raise e\n",
    "\n",
    "# -----------------------\n",
    "# Save DPO model\n",
    "# -----------------------\n",
    "print(\"Saving DPO model...\")\n",
    "dpo_trainer.save_model(\"./dpo_model\")\n",
    "tokenizer.save_pretrained(\"./dpo_model\")\n",
    "\n",
    "print(\"Finished: Complete SFT â†’ DPO pipeline!\")\n",
    "print(\"Final model saved in ./dpo_model/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpo_mtw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
